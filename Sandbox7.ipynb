{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "basic-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "framed-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _open_cleaned_file(file, folder):\n",
    "    \"\"\"Reads the cleaned file. Adds it to the output dataframe we are working with form here on .\n",
    "    Returns the list of the leading nan values in the list. Nan_list may be empty\n",
    "\n",
    "    Args:\n",
    "        file (str): file name\n",
    "\n",
    "    Returns:\n",
    "        List: List over nan values\n",
    "    \"\"\"\n",
    "    nan_list = []\n",
    "    file_object = open(folder + file, \"r\")\n",
    "    file_object.readline()\n",
    "    for line in file_object:\n",
    "        if line[:2] == \"#*\":\n",
    "            df_output = pd.read_csv(file_object, delim_whitespace=True, header=0, index_col=[0, 1, 2])\n",
    "            break\n",
    "        nan_list.append(eval(line[2:]))\n",
    "\n",
    "    file_object.close()\n",
    "    return nan_list, df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sought-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_nan(df_output, col):\n",
    "    \"\"\"Returns a list of all nan indexes in the dataframe.txt\n",
    "    Redundant\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: index array all nan indexes\n",
    "    \"\"\"\n",
    "    # If it finds nan values in the data set, it returns the location as a tuple of the nan values\n",
    "    # Potential issues is long periods of NaN values\n",
    "    nan_values = df_output[col].isnull() # == nan_ident                  # Creates a series\n",
    "    nr_nan_values = nan_values.value_counts()                                   # Counts values in a truth array\n",
    "    if True in nr_nan_values.index:                                             # Check if any value is true\n",
    "        loc_nan = nan_values[nan_values == True].index.values                   # Creates a list of tuples of NaN locations\n",
    "        return loc_nan                                                          # Returns above list\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worst-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_hyd_year(df_output):\n",
    "        \"\"\"Adds the hydrological year to the dataframe as a new column and sets it as column number 1, index 0\n",
    "        \"\"\"\n",
    "        def assign_wy(row):\n",
    "            if row.name[1] > 8:\n",
    "                return(int(row.name[0] + 1))\n",
    "            else:\n",
    "                return(int(row.name[0]))\n",
    "        df_output['WY'] = df_output.apply(lambda x: assign_wy(x), axis=1)     # Adds column\n",
    "        df_output = df_output[[\"WY\", \"discharge\"]]                             # Sorts order\n",
    "        return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "horizontal-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _segement_dataframe_baseflow(nan_list, df_output):\n",
    "    \"\"\"Creates a new column in the dataframe including the baseflow seperated values\n",
    "\n",
    "    Args:\n",
    "        nan_list (List): List of tuples of nan placements, output of the self._congregate_nan_list(x)\n",
    "    \"\"\"\n",
    "    # Start of loop\n",
    "    start = 0\n",
    "    reflect = 30\n",
    "    length_of_nan = []\n",
    "    for tup in nan_list:\n",
    "        length_of_nan.append(tup[2])\n",
    "\n",
    "        input_array = df_output[\"discharge\"].iloc[start:tup[0]].to_numpy()        # creates the input array of valid length\n",
    "        baseflow = _baseflow_seperation(input_array)                              # finds the baseflow seperation from said input array\n",
    "        df_output[\"Qb\"].iloc[start:tup[0]] = baseflow                             # Adds baseflow to the dataframe\n",
    "\n",
    "        start = tup[1] + 1\n",
    "    # End of loop\n",
    "    # Handles the tail of the dataframe, this should always be valid, but this makes sure it is valid.\n",
    "    # If not valid set end to nan values.\n",
    "    tail = len(df_output[\"discharge\"].iloc[start:]) > reflect\n",
    "    if tail:                                                                      # Fixes the tail if it exists.\n",
    "        input_array = df_output[\"discharge\"].iloc[start:].to_numpy()              # Creates the final array and handles that\n",
    "        baseflow = _baseflow_seperation(input_array)                              # finds the baseflow seperation from said input array\n",
    "        df_output[\"Qb\"].iloc[start:] = baseflow                                   # Adds baseflow to the dataframe\n",
    "    else:\n",
    "        df_output[\"Qb\"].iloc[start:] = np.nan(len(df_output[\"Qb\"].iloc[start:]), np.nan)\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "earned-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _baseflow_seperation(q, alpha=0.925, reflect=30):\n",
    "    \"\"\"Calculates the baseflow for a set dataframe using the Lyne-Hollick method.\n",
    "    This function is based on the implementation recommended by Tony Ladson (2013) and his accompanying R-implementation avaiable at\n",
    "    - DOI:       10.7158/W12-028.2013.17.1.\n",
    "    - Code:      https://github.com/TonyLadson/BaseflowSeparation_LyneHollick \n",
    "    - Blogpost:  https://tonyladson.wordpress.com/2013/10/01/a-standard-approach-to-baseflow-separation-using-the-lyne-and-hollick-filter/\n",
    "\n",
    "    Date for implementation: 2022/02/23 \n",
    "    Crossreferenced last:    2022/02/23\n",
    "\n",
    "    Freedoms have been taken in how data is handled and worked with as a different langauge is used.\n",
    "    The essence of the recommended implementation is followed, while not all functions may be working.\n",
    "    Some unnecessary implementation is done to stay true to the original R-implementation used as base. \n",
    "\n",
    "    The input array is the array for which the baseflow needs to be calculated, sequencing and slicing happens prior to this. \n",
    "\n",
    "    Args:\n",
    "        q (np.ndarray): Input streamflow data in the form of a numpy array\n",
    "        alpha (float, optional): Digital filter parameter based on the local and/or literature values for this parameter. Defaults to 0.925.\n",
    "        reflect (int, optional): Amount of values to reflect. Defaults to recommended value for daily streamflow data. Defaults to 30.\n",
    "\n",
    "    Returns:\n",
    "        Tuple(np.ndarray): baseflow values for the streamflow.\n",
    "    \"\"\"\n",
    "    # TODO: Look for simplifications if there is no need to save values they can be removed.\n",
    "\n",
    "    # Expecting q to be a vector from numpy\n",
    "    if not isinstance(q, np.ndarray):\n",
    "        print(\"q must be a np.ndarray, it is \", type(q))\n",
    "#         exit()\n",
    "\n",
    "    if reflect >= len(q):\n",
    "        print(\"Data set must be longer than the reflect period. Exiting...\")\n",
    "#         exit()\n",
    "\n",
    "    if alpha < 0 or alpha >= 1:\n",
    "        print(\"alpha must be between 0 and 1. Exiting...\")\n",
    "#         exit()\n",
    "\n",
    "    def first_pass(q, a) -> pd.DataFrame:\n",
    "        \"\"\"Firsts forward pass of the dataframe, needs to be handled slightly differently than the later forward pass, due to being implemented as a dataframe.\n",
    "        Choice of this was to avoid indexing issues when implementing.\n",
    "\n",
    "        Args:\n",
    "            q (np.ndarray): streamflow values\n",
    "            a (float): digital filter parameter\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe with columns [\"qf\", \"qb\"], which are the seperated quickflow and baseflow for use in the following passes\n",
    "        \"\"\"\n",
    "        b = 0.5 * (1 + a)\n",
    "        qf = np.zeros(len(q))  # Empty quickflow\n",
    "        qf[0] = q[0]\n",
    "        for i in range(1, len(qf)):\n",
    "            qf[i] = a * qf[i - 1] + b * (q[i] - q[i - 1])\n",
    "\n",
    "        qb1 = np.where(qf > 0, q - qf, q)\n",
    "\n",
    "        return pd.DataFrame({\"qf\": qf, \"qb\": qb1})\n",
    "\n",
    "    def backwards_pass(q, a) -> pd.DataFrame:\n",
    "        \"\"\"Backwards pass in the Lyne-Hollick method\n",
    "\n",
    "        Args:\n",
    "            q (pd.DataFrame): Dataframe with columns [\"qf\", \"qb\"], which are the seperated quickflow and baseflow for use in the following passes\n",
    "            a (float): digital filter parameter\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe with columns [\"qf\", \"qb\"], which are the seperated quickflow and baseflow for use in the following passes\n",
    "        \"\"\"\n",
    "        n = len(q[\"qb\"])\n",
    "        qb = q[\"qb\"]\n",
    "        b = 0.5 * (1 + a)\n",
    "\n",
    "        qf = np.zeros(n)  # Empty array\n",
    "        qf[-1] = qb.iloc[-1]\n",
    "\n",
    "        for i in range(n - 2, 0, -1):\n",
    "            qf[i] = a * qf[i + 1] + b * (qb.iloc[i] - qb.iloc[i + 1])\n",
    "\n",
    "        qb2 = np.where(qf > 0, qb - qf, qb)\n",
    "\n",
    "        return pd.DataFrame({\"qf\": qf, \"qb\": qb2})\n",
    "\n",
    "    def forward_pass(q, a) -> pd.DataFrame:\n",
    "        \"\"\"Forward pass, similar to the first pass, but uses dataframes instead\n",
    "\n",
    "        Args:\n",
    "            q (pd.DataFrame): Dataframe with columns [\"qf\", \"qb\"], which are the seperated quickflow and baseflow for use in the following passes\n",
    "            a (float): digital filter parameter\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe with columns [\"qf\", \"qb\"], which are the seperated quickflow and baseflow for use in the following passes\n",
    "        \"\"\"\n",
    "        n = len(q[\"qb\"])\n",
    "        qb = q[\"qb\"]\n",
    "        b = 0.5 * (1 + a)\n",
    "\n",
    "        qf = np.zeros(n)  # Empty array\n",
    "        qf[0] = qb.iloc[0]\n",
    "\n",
    "        for i in range(1, n):\n",
    "            qf[i] = a * qf[i - 1] + b * (qb.iloc[i] - qb.iloc[i - 1])\n",
    "\n",
    "        qb2 = np.where(qf > 0, qb - qf, qb)\n",
    "\n",
    "        return pd.DataFrame({\"qf\": qf, \"qb\": qb2})\n",
    "\n",
    "    q_in = np.pad(q, (reflect, reflect), mode=\"reflect\")  # Pad the dataset\n",
    "\n",
    "    # First pass always needed\n",
    "    df_tmp = first_pass(q_in, alpha)\n",
    "\n",
    "    df_tmp = backwards_pass(df_tmp, alpha)\n",
    "\n",
    "    df_tmp = forward_pass(df_tmp, alpha)\n",
    "\n",
    "    qb = df_tmp[\"qb\"][reflect:-reflect].to_numpy()\n",
    "\n",
    "    qb[qb < 0] = 0                  # Set values less than zero to zero\n",
    "\n",
    "    return qb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exempt-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nanindex(df_output, col, nan_ident=-9999):\n",
    "    \"\"\"Works on the current dataframe\n",
    "    Retunrs as follows: # Start_Nan End_Nan Length_Nan Length_NoNan\n",
    "    Start_Nan is the index value of the first nan values encountered\n",
    "    End_Nan is the first non nan value after\n",
    "    Length_Nan is the length of the sequence of Nan values\n",
    "    Length_NoNan is the prior length of no nan values\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple]: Tuple of the above property\n",
    "    \"\"\"\n",
    "    data = df_output[col].to_numpy()\n",
    "    l = []\n",
    "    i = 0\n",
    "    vq = 0\n",
    "    \n",
    "    while i < len(data):\n",
    "        val = data[i]\n",
    "        if val == nan_ident:\n",
    "            v1 = i\n",
    "            for j, val in enumerate(data[i:]):\n",
    "                if val != nan_ident:\n",
    "                    v2 = i + j\n",
    "                    break\n",
    "            l.append((v1, v2, v2 - v1, v1 - vq))\n",
    "            vq = v2\n",
    "\n",
    "            i += j\n",
    "        i += 1\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "still-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _congregate_nan_list(nan_list):\n",
    "    \"\"\"Congregates the nan_lists.\n",
    "    In most cases this functions does nothing.\n",
    "    If Length_NoNan is shorter than reflect then it will congregate the sequence and essentially treat the data even though present as missing.\n",
    "    So if Length_NoNan < reflect then we ignore the entire non nan segment and following calculations.\n",
    "\n",
    "    Potential issue with this is if there is a single nan followed by n<reflect non-nans with a following nan. Where an entire segment is thrown when not needed.\n",
    "    To counter his an interpolation may be used on the single nan or few nans to have a better dataset.\n",
    "\n",
    "    Args:\n",
    "        nan_list (List): List of nan values generated earlier by the self._nanindex function\n",
    "\n",
    "    Returns:\n",
    "        List: New list of nan tuples that are congregated.\n",
    "    \"\"\"\n",
    "    # Here add handling of nan-values\n",
    "    # Start_Nan End_Nan Length_Nan Length_NoNan\n",
    "    # TODO: Make into function\n",
    "    # TODO: Cleans up the nan_indicies. Do not want to do this before clean so I can manually check files.\n",
    "    # TODO: Used to create merged nan_indicies\n",
    "    if len(nan_list) == 0:\n",
    "        return [], nan_list\n",
    "    \n",
    "    weights = []            # used in the weighting of the function, should contain length of data series\n",
    "    reflect = 30\n",
    "\n",
    "    start_index = []\n",
    "    prev_nonan = []\n",
    "    new_nan_list = []\n",
    "    \n",
    "    sequence = False\n",
    "    \n",
    "    \n",
    "    for nan_indicies in nan_list:\n",
    "        no_nan = nan_indicies[3]\n",
    "        \n",
    "        if sequence and no_nan > reflect:\n",
    "#             print(\"New tuple - 1\")\n",
    "            new_tuple = (start_index[0], start_index[-1], start_index[-1] - start_index[0], prev_nonan[0])\n",
    "            new_nan_list.append(new_tuple)\n",
    "            \n",
    "            start_index = []\n",
    "            prev_nonan = []\n",
    "            \n",
    "            sequence = False\n",
    "#         print(nan_indicies)\n",
    "        start_index.append(nan_indicies[0])\n",
    "        start_index.append(nan_indicies[1])\n",
    "        \n",
    "        prev_nonan.append(no_nan)\n",
    "\n",
    "        if no_nan < reflect:\n",
    "            continue\n",
    "        else: \n",
    "            sequence = True\n",
    "        if nan_indicies == nan_list[-1]:\n",
    "#             print(\"New tuple - 2\")\n",
    "            new_tuple = (start_index[0], start_index[-1], start_index[-1] - start_index[0], prev_nonan[0])\n",
    "            new_nan_list.append(new_tuple)\n",
    "    return weights, new_nan_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "higher-prior",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigurd/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "2.265.q\n",
      "[(9253, 10349, 1096, 9253)]\n",
      "2.268.q\n",
      "[(9619, 9932, 313, 9619)]\n",
      "2.279.q\n",
      "[]\n",
      "2.479.q\n",
      "[(122, 1066, 944, 122), (1135, 2516, 1381, 69), (2595, 2814, 219, 79)]\n",
      "2.633.q\n",
      "[(6698, 6699, 1, 6698), (6919, 6927, 8, 220), (7022, 7023, 1, 95)]\n"
     ]
    }
   ],
   "source": [
    "folder = \"../GEO3000/code/data/discharge_data_100_cleaned/\"\n",
    "files = [\"2.11.q\", \"2.265.q\", \"2.268.q\", \"2.279.q\", \"2.479.q\", \"2.633.q\"]\n",
    "for file in files:\n",
    "    print(file)\n",
    "    nan_list, df_output = _open_cleaned_file(file, folder)\n",
    "    \n",
    "    df_output = _add_hyd_year(df_output)\n",
    "    \n",
    "    df_output[\"Qb\"] = pd.Series(dtype=np.float64)              # Adds a empty column for the baseflow\n",
    "    \n",
    "    weights, nan_list = _congregate_nan_list(nan_list)\n",
    "      \n",
    "    df_output = _segement_dataframe_baseflow(nan_list, df_output)                     # Segments the dataframe and does the baseflow seperation\n",
    "    print(nan_list)\n",
    "#     print(_check_nan(df_output, \"Qb\", np.nan))\n",
    "    if file == \"2.633.q\":\n",
    "        df_test = df_output.copy()\n",
    "\n",
    "    df_output[\"nn\"] = df_output[\"Qb\"].isnull()\n",
    "    df_grouped_null = df_output.groupby(\"WY\")[\"nn\"].sum()\n",
    "#     display(df_grouped_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "numeric-stopping",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998\n",
      "1999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discharge</th>\n",
       "      <th>Qb</th>\n",
       "      <th>BFI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>472.744696</td>\n",
       "      <td>187.920541</td>\n",
       "      <td>0.397510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>587.515731</td>\n",
       "      <td>247.943745</td>\n",
       "      <td>0.422021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>534.418540</td>\n",
       "      <td>224.558832</td>\n",
       "      <td>0.420193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>642.928929</td>\n",
       "      <td>337.380699</td>\n",
       "      <td>0.524756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>437.202947</td>\n",
       "      <td>164.713263</td>\n",
       "      <td>0.376743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>722.678205</td>\n",
       "      <td>314.230058</td>\n",
       "      <td>0.434813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>667.985530</td>\n",
       "      <td>315.128265</td>\n",
       "      <td>0.471759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>671.874025</td>\n",
       "      <td>312.086318</td>\n",
       "      <td>0.464501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>1213.459365</td>\n",
       "      <td>642.836344</td>\n",
       "      <td>0.529755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>631.438153</td>\n",
       "      <td>312.465076</td>\n",
       "      <td>0.494847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>569.550126</td>\n",
       "      <td>284.852235</td>\n",
       "      <td>0.500135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>430.904806</td>\n",
       "      <td>186.489714</td>\n",
       "      <td>0.432786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>496.149966</td>\n",
       "      <td>267.394926</td>\n",
       "      <td>0.538940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>499.174583</td>\n",
       "      <td>250.261801</td>\n",
       "      <td>0.501351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>460.069881</td>\n",
       "      <td>206.712997</td>\n",
       "      <td>0.449308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>677.043651</td>\n",
       "      <td>390.445509</td>\n",
       "      <td>0.576692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>342.714889</td>\n",
       "      <td>187.058517</td>\n",
       "      <td>0.545814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>666.373563</td>\n",
       "      <td>325.525834</td>\n",
       "      <td>0.488504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-89385.926133</td>\n",
       "      <td>324.146740</td>\n",
       "      <td>0.534972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-9084.690687</td>\n",
       "      <td>439.657329</td>\n",
       "      <td>0.431064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>843.424873</td>\n",
       "      <td>431.551856</td>\n",
       "      <td>0.511666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>1078.175816</td>\n",
       "      <td>557.119240</td>\n",
       "      <td>0.516724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>665.441559</td>\n",
       "      <td>338.522374</td>\n",
       "      <td>0.508718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>479.717970</td>\n",
       "      <td>208.176923</td>\n",
       "      <td>0.433957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>402.746377</td>\n",
       "      <td>168.871982</td>\n",
       "      <td>0.419301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>644.054172</td>\n",
       "      <td>303.120547</td>\n",
       "      <td>0.470644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>568.798267</td>\n",
       "      <td>229.056818</td>\n",
       "      <td>0.402703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>858.555468</td>\n",
       "      <td>365.961866</td>\n",
       "      <td>0.426253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>775.248484</td>\n",
       "      <td>361.700316</td>\n",
       "      <td>0.466560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>617.865307</td>\n",
       "      <td>283.641784</td>\n",
       "      <td>0.459067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>511.277651</td>\n",
       "      <td>177.825685</td>\n",
       "      <td>0.347806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>523.351815</td>\n",
       "      <td>232.885257</td>\n",
       "      <td>0.444988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>711.677230</td>\n",
       "      <td>339.274781</td>\n",
       "      <td>0.476726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>657.013131</td>\n",
       "      <td>300.211604</td>\n",
       "      <td>0.456934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>762.358888</td>\n",
       "      <td>287.827438</td>\n",
       "      <td>0.377548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>695.084378</td>\n",
       "      <td>326.560593</td>\n",
       "      <td>0.469814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>566.064566</td>\n",
       "      <td>220.355930</td>\n",
       "      <td>0.389277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>264.076773</td>\n",
       "      <td>119.415978</td>\n",
       "      <td>0.452202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>535.290439</td>\n",
       "      <td>254.335392</td>\n",
       "      <td>0.475135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>504.801064</td>\n",
       "      <td>197.774871</td>\n",
       "      <td>0.391788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>823.697203</td>\n",
       "      <td>422.046396</td>\n",
       "      <td>0.512381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         discharge          Qb       BFI\n",
       "WY                                      \n",
       "1980    472.744696  187.920541  0.397510\n",
       "1981    587.515731  247.943745  0.422021\n",
       "1982    534.418540  224.558832  0.420193\n",
       "1983    642.928929  337.380699  0.524756\n",
       "1984    437.202947  164.713263  0.376743\n",
       "1985    722.678205  314.230058  0.434813\n",
       "1986    667.985530  315.128265  0.471759\n",
       "1987    671.874025  312.086318  0.464501\n",
       "1988   1213.459365  642.836344  0.529755\n",
       "1989    631.438153  312.465076  0.494847\n",
       "1990    569.550126  284.852235  0.500135\n",
       "1991    430.904806  186.489714  0.432786\n",
       "1992    496.149966  267.394926  0.538940\n",
       "1993    499.174583  250.261801  0.501351\n",
       "1994    460.069881  206.712997  0.449308\n",
       "1995    677.043651  390.445509  0.576692\n",
       "1996    342.714889  187.058517  0.545814\n",
       "1997    666.373563  325.525834  0.488504\n",
       "1998 -89385.926133  324.146740  0.534972\n",
       "1999  -9084.690687  439.657329  0.431064\n",
       "2000    843.424873  431.551856  0.511666\n",
       "2001   1078.175816  557.119240  0.516724\n",
       "2002    665.441559  338.522374  0.508718\n",
       "2003    479.717970  208.176923  0.433957\n",
       "2004    402.746377  168.871982  0.419301\n",
       "2005    644.054172  303.120547  0.470644\n",
       "2006    568.798267  229.056818  0.402703\n",
       "2007    858.555468  365.961866  0.426253\n",
       "2008    775.248484  361.700316  0.466560\n",
       "2009    617.865307  283.641784  0.459067\n",
       "2010    511.277651  177.825685  0.347806\n",
       "2011    523.351815  232.885257  0.444988\n",
       "2012    711.677230  339.274781  0.476726\n",
       "2013    657.013131  300.211604  0.456934\n",
       "2014    762.358888  287.827438  0.377548\n",
       "2015    695.084378  326.560593  0.469814\n",
       "2016    566.064566  220.355930  0.389277\n",
       "2017    264.076773  119.415978  0.452202\n",
       "2018    535.290439  254.335392  0.475135\n",
       "2019    504.801064  197.774871  0.391788\n",
       "2020    823.697203  422.046396  0.512381"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_test.copy()\n",
    "\n",
    "\n",
    "df_test = df_test.groupby(\"WY\").sum()\n",
    "df_test[\"BFI\"] = df_test[\"Qb\"]/df_test[\"discharge\"]\n",
    "\n",
    "def _pad_year(x):\n",
    "    \"\"\"\n",
    "    Takes the array of the baseflow and returns the indexes of starting and ending nan values\n",
    "    \"\"\"\n",
    "    x = np.hstack([ [False], x, [False] ])  # padding\n",
    "    d = np.diff(x.astype(int))\n",
    "    starts = np.where(d == 1)[0]\n",
    "    ends = np.where(d == -1)[0]\n",
    "    return starts, ends\n",
    "\n",
    "def _weighted_avg(x, y, s, e):\n",
    "    \"\"\"\n",
    "    Computes the weighted avg of the baseflow based on the starts and ends of the nan vlaues\n",
    "    \"\"\"\n",
    "    n1 = 0\n",
    "    n2 = 0\n",
    "    w = []\n",
    "    calc = []\n",
    "    for i,j in zip(starts, ends):\n",
    "        w.append(i - n1)\n",
    "        calc.append(sum(x[n1:i])/sum(y[n1:i]))\n",
    "        n1 = j\n",
    "        \n",
    "    qb = 0    \n",
    "    for i in range(len(w)):\n",
    "        qb += calc[i]*w[i]\n",
    "    bfi = qb/sum(w)\n",
    "    return bfi\n",
    "\n",
    "# Find nan locations in the entire series of the baseflow values\n",
    "df_tmp[\"nn\"] = df_tmp[\"Qb\"].isnull()\n",
    "\n",
    "# Group the baseflow nans so it is per years\n",
    "df_grouped = df_tmp.groupby(\"WY\")[\"nn\"].sum()\n",
    "# Find all years where more than 5% of the baseflow values are missing. Change 19 if more values are acceptable\n",
    "reqd_index_drop = df_grouped[df_grouped>=19].index.tolist()\n",
    "# Find all years where there is more than 1 missing value\n",
    "reqd_index = df_grouped[df_grouped > 0].index.tolist()\n",
    "\n",
    "# reqd_index_drop is the years to drop regardless as too much data is missing\n",
    "# reqd_index is the years where a weighted avg will be used. \n",
    "\n",
    "# Removes overlap between the two sets of years\n",
    "for element in reqd_index_drop:\n",
    "    if element in reqd_index:\n",
    "        reqd_index.remove(element)\n",
    "reqd_index = [1998, 1999]\n",
    "# Loop over all years that are of interest\n",
    "for year in reqd_index:\n",
    "    print(year)\n",
    "    # Extract that specific year\n",
    "    tmp = df_tmp[df_tmp[\"WY\"] == year]\n",
    "    # Convert to numpy array\n",
    "    tmp_n = tmp[\"nn\"].to_numpy()\n",
    "    # Computes the starts and ends of the nan values\n",
    "    starts, ends = _pad_year(tmp_n)\n",
    "    # Computes the weighted avg\n",
    "    wa = _weighted_avg(tmp[\"Qb\"], tmp[\"discharge\"], starts, ends)\n",
    "    df_test[\"BFI\"].loc[year] = wa\n",
    "df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3810jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
